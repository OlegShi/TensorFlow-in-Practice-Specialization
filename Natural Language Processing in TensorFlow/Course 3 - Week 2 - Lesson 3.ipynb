{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Course 3 - Week 2 - Lesson 3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"P-AhVYeBWgQ3","colab_type":"code","colab":{}},"source":["# NOTE: PLEASE MAKE SURE YOU ARE RUNNING THIS IN A PYTHON3 ENVIRONMENT\n","\n","import tensorflow as tf\n","print(tf.__version__)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hd4c-LEHCFZN","colab_type":"code","colab":{}},"source":["# Uncomment and run this if you don't have TensorFlow 2.0x [Check for latest 2.0 instructions at https://www.tensorflow.org/versions/r2.0/api_docs/python/tf]\n","#!pip install tensorflow==2.0.0-beta0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wm9S3T8-9H4q","colab_type":"code","colab":{}},"source":["# Double check TF 2.0x is installed. If you ran the above block, there was a \n","# 'reset all runtimes' button at the bottom that you needed to press\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_IoM4VFxWpMR","colab_type":"code","colab":{}},"source":["# If the import fails, run this\n","# !pip install -q tensorflow-datasets\n","\n","import tensorflow_datasets as tfds\n","imdb, info = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wHQ2Ko0zl7M4","colab_type":"code","colab":{}},"source":["train_data, test_data = imdb['train'], imdb['test']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqGRSe_eCdOz","colab_type":"code","colab":{}},"source":["tokenizer = info.features['text'].encoder"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6b_EraCDLOh","colab_type":"code","colab":{}},"source":["print(tokenizer.subwords)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPl2BXhYEHRP","colab_type":"code","colab":{}},"source":["sample_string = 'TensorFlow, from basics to mastery'\n","\n","tokenized_string = tokenizer.encode(sample_string)\n","print ('Tokenized string is {}'.format(tokenized_string))\n","\n","original_string = tokenizer.decode(tokenized_string)\n","print ('The original string: {}'.format(original_string))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_3t7vvNLEZml","colab_type":"code","colab":{}},"source":["for ts in tokenized_string:\n","  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5NEpdhb8AxID","colab_type":"code","colab":{}},"source":["embedding_dim = 64\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(6, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkt8c5dNuUlT","colab_type":"code","colab":{}},"source":["num_epochs = 10\n","\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","history = model.fit(train_data, epochs=num_epochs, validation_data=test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-_rMnm7WxQGT","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","\n","def plot_graphs(history, string):\n","  plt.plot(history.history[string])\n","  plt.plot(history.history['val_'+string])\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(string)\n","  plt.legend([string, 'val_'+string])\n","  plt.show()\n","  \n","plot_graphs(history, \"accuracy\")\n","plot_graphs(history, \"loss\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qACq5FLzTW4A","colab_type":"code","colab":{}},"source":["e = model.layers[0]\n","weights = e.get_weights()[0]\n","print(weights.shape) # shape: (vocab_size, embedding_dim)\n","\n","import io\n","\n","out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n","out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n","for word_num in range(1, tokenizer.vocab_size):\n","  word = tokenizer.decode([word_num])\n","  embeddings = weights[word_num]\n","  out_m.write(word + \"\\n\")\n","  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n","out_v.close()\n","out_m.close()\n","\n","\n","try:\n","  from google.colab import files\n","except ImportError:\n","  pass\n","else:\n","  files.download('vecs.tsv')\n","  files.download('meta.tsv')"],"execution_count":0,"outputs":[]}]}
